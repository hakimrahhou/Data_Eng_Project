# Sujet de travaux pratiques "Introduction √† la data ing√©nierie"

Le but de ce projet est de cr√©er un pipeline ETL d'ingestion, de transformation et de stockage de donn√©es pour mettre en pratique les connaissances acquises lors du cours d'introduction √† la data ing√©nierie. Ce sujet pr√©sent√© propose d'utiliser les donn√©es d'utilisation des bornes de v√©los open-sources et "temps r√©el" dans les grandes villes de France.

Le sujet propose une base qui est un pipeline ETL complet qui couvre la r√©cup√©ration, le stockage et la transformation d'une partie des donn√©es de la ville de Paris.

Le but du sujet de travaux pratiques est d'ajouter √† ce pipeline des donn√©es de consolidation, de dimensions et de faits pour la ville de Paris, ainsi que les donn√©es provenant d'autres grandes villes de France. Ces donn√©es sont disponibles pour les villes de Nantes, de Toulouse ou encore de Strasbourg. Il faudra aussi enrichir ces donn√©es avec les donn√©es descriptives des villes de France, via une API de l'√âtat fran√ßais open-source.


## Architecture du projet

Data_Eng_Project/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py
‚îÇ   ‚îú‚îÄ‚îÄ data_consolidation.py
‚îÇ   ‚îú‚îÄ‚îÄ data_agregation.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw_data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ YYYY-MM-DD/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ paris_realtime_bicycle_data.json
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ nantes_realtime_bicycle_data.json
‚îÇ   ‚îú‚îÄ‚îÄ duckdb/
‚îÇ       ‚îî‚îÄ‚îÄ mobility_analysis.duckdb
‚îÇ
‚îú‚îÄ‚îÄ data/sql_statements/
‚îÇ   ‚îú‚îÄ‚îÄ create_consolidate_tables.sql
‚îÇ   ‚îú‚îÄ‚îÄ create_agregate_tables.sql
‚îÇ
‚îî‚îÄ‚îÄ README.md

## Ex√©cution du pipeline

git clone <repo>
cd Data_Eng_Project

python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# OU
.\.venv\Scripts\activate   # Windows

pip install -r requirements.txt

python src/main.py

## üì° Sch√©ma du pipeline
        +---------------+
        |  Ingestion    |
        | (APIs Paris & |
        |   Nantes)     |
        +-------+-------+
                |
       raw JSON files
                |
        +-------v-------+
        | Consolidation |
        |  (DuckDB)     |
        +-------+-------+
                |
        DIM + FACT tables
                |
        +-------v-------+
        |  Agr√©gation   |
        |   (DW)        |
        +---------------+

## Explication du code existant

Le projet est d√©coup√© en 3 parties :

1. Un fichier python pour r√©cup√©rer et stocker les donn√©es dans des fichiers localement

2. Un fichier python pour consolider les donn√©es et faire un premier load dans une base de donn√©es type data-warehouse

3. Un fichier python pour agr√©ger les donn√©es et cr√©er une mod√©lisation de type dimensionnelle

### Ingestion des donn√©es

```python
def get_paris_realtime_bicycle_data():
    url = "https://opendata.paris.fr/api/explore/v2.1/catalog/datasets/velib-disponibilite-en-temps-reel/exports/json"
    response = requests.request("GET", url)
    serialize_data(response.text, "paris_realtime_bicycle_data.json")

def serialize_data(raw_json: str, file_name: str):
    today_date = datetime.now().strftime("%Y-%m-%d")
    if not os.path.exists(f"data/raw_data/{today_date}"):
        os.makedirs(f"data/raw_data/{today_date}")
    with open(f"data/raw_data/{today_date}/{file_name}", "w") as fd:
        fd.write(raw_json)
```

Ces fonctions python sont assez simples. Elles r√©cup√®rent les donn√©es sur une API open-source, et les stockent dans un fichier json localement. Ces fonctions sont dans le fichier python `data_ingestion.py`.

### Consolidation des donn√©es

**Duckdb** est une base de donn√©es de type data-warehouse que l'on peut utiliser localement, car elle charge les donn√©es en "in-memory" ou dans un fichier local. C'est l'√©quivalent de SQLite, mais pour des bases de donn√©es de type data-warehouse. Cr√©er une connexion duckdb permet de "cr√©er" une database et d'interagir avec comme avec un vrai data-warehouse. C'est parfait pour des projets de ce type. Plus d'informations sur le site officiel de duckdb : https://duckdb.org/. N'oubliez pas d'installer le CLI DuckDB via cette documentation : https://duckdb.org/install/?platform=macos&environment=cli. Le CLI vous sera utile plus tard.

Dans le fichier `data_consolidation.py` on trouve une fonction qui permet de cr√©er les tables dans une base de donn√©es **duckdb**. On utilise le fichier `create_consolidate_tables.sql` pour d√©finir les sch√©mas des tables. Vous ne devriez pas avoir √† modifier les sch√©mas des tables, mais vous pouvez le faire si vous voyez une optimisation √† faire ou si le sch√©ma est contraignant pour vous pour la r√©alisation de ce TP.

```python
def create_consolidate_tables():
    con = duckdb.connect(database = "data/duckdb/mobility_analysis.duckdb", read_only = False)
    with open("data/sql_statements/create_consolidate_tables.sql") as fd:
        statements = fd.read()
    for statement in statements.split(";"):
        print(statement)
        con.execute(statement)
```

Une fois les tables cr√©√©es, on peut lancer les fonctions de consolidations. Un exemple est donn√© dans le fichier `data_consolidation.py` :

```python
def consolidate_city_data():

    con = duckdb.connect(database = "data/duckdb/mobility_analysis.duckdb", read_only = False)
    data = {}

    with open(f"data/raw_data/{today_date}/paris_realtime_bicycle_data.json") as fd:
        data = json.load(fd)

    raw_data_df = pd.json_normalize(data)
    raw_data_df["nb_inhabitants"] = None

    city_data_df = raw_data_df[[
        "code_insee_commune",
        "nom_arrondissement_communes",
        "nb_inhabitants"
    ]]
    city_data_df.rename(columns={
        "code_insee_commune": "id",
        "nom_arrondissement_communes": "name"
    }, inplace=True)
    city_data_df.drop_duplicates(inplace = True)

    city_data_df["created_date"] = date.today()
    
    con.execute("INSERT OR REPLACE INTO CONSOLIDATE_CITY SELECT * FROM city_data_df;")
```

Explication de cette fonction :

- On commence par cr√©er une connexion √† la base duckdb (`read_only = False`) car on va ins√©rer les donn√©es dans la base.

- On charge les donn√©es depuis les fichiers JSON locaux que l'on a ing√©r√©s dans l'√©tape pr√©c√©dente dans un dataframe Pandas.

- On travaille notre dataframe pour :
  - renommer les colonnes
  - supprimer les colonnes inutiles
  - ajouter des colonnes qui sont attendues par la table `CONSOLIDATE_CITY` dans notre base de donn√©es (ici `nb_inhabitants`)

- On ins√®re les donn√©es dans la base directement depuis le dataframe (fonctionnalit√© de duckdb, voir la documentation).

**ATTENTION** : Lors de l'insertion de donn√©es dans une table duckdb avec une requ√™te SQL `INSERT OR REPLACE INTO CONSOLIDATE_CITY SELECT * FROM city_data_df;`, il faut s'assurer que :

- votre dataframe contient le m√™me nombre de colonnes que la table dans la base de donn√©es
- les colonnes dans votre dataframe et dans la table doivent √™tre dans le m√™me ordre

**ATTENTION 2** : Les donn√©es sont historis√©es dans les tables de consolidation (d'o√π la pr√©sence des colonnes `created_date` et `id` ou `station_id`). C'est uniquement un choix de conception. Vous pouvez changer ce comportement et supprimer / recharger les donn√©es √† chaque fois.

Les autres fonctions de consolidation pour les autres tables devraient √™tre similaires.

### Agr√©gation des donn√©es

Dans le fichier `data_agregation.py` on trouve une fonction qui permet de cr√©er les tables dans une base de donn√©es **duckdb**. On utilise le fichier `create_agregate_tables.sql` pour d√©finir les sch√©mas des tables. Ces tables repr√©sentent une mod√©lisation dimensionnelle simple :

- Deux tables de dimensions : `dim_city` et `dim_station` qui repr√©sentent les donn√©es descriptives des villes et des stations de v√©los en libre-service.

- Une table de faits : `fact_station_statement` qui repr√©sente les relev√©s de disponibilit√© des v√©los dans les stations.

Vous ne devriez pas avoir √† modifier les sch√©mas des tables, mais vous pouvez le faire si vous voyez une optimisation ou si le sch√©ma est contraignant pour vous pour la r√©alisation de ce TP.

```python
def create_agregate_tables():
    con = duckdb.connect(database = "data/duckdb/mobility_analysis.duckdb", read_only = False)
    with open("data/sql_statements/create_agregate_tables.sql") as fd:
        statements = fd.read()
    for statement in statements.split(";"):
        print(statement)
        con.execute(statement)
```

Une fois les tables cr√©√©es, on peut lancer les autres fonctions d'agr√©gation. Pour les tables de dimensions, les fonctions sont assez simples. Comme vous pouvez le voir dans le fichier exemple `data_agregation.py`:

```python
def agregate_dim_city():
    con = duckdb.connect(database = "data/duckdb/mobility_analysis.duckdb", read_only = False)
    
    sql_statement = """
    INSERT OR REPLACE INTO DIM_CITY
    SELECT 
        ID,
        NAME,
        NB_INHABITANTS
    FROM CONSOLIDATE_CITY
    WHERE CREATED_DATE = (SELECT MAX(CREATED_DATE) FROM CONSOLIDATE_CITY);
    """

    con.execute(sql_statement)
```

La fonction pour la table `fact_station_statement` sera plus complexe: elle devra g√©rer les jointures avec les autres tables pour que les donn√©es soient analysables avec les donn√©es descriptives des tables de dimensions.

### Le fichier main.py

Le fichier `main.py` contient le code principal du processus et ex√©cute s√©quentiellement les diff√©rentes fonctions expliqu√©es plus haut.

### Points d'attention:

Les √©tapes de ce pipeline seront ex√©cut√©s s√©quentiellement (voir le fichier `main.py`) car :

- On ne peut pas faire de l'orchestration facilement dans les environnements locaux de Polytech

- Ce n'est pas possible d'avoir des connexions concurrentes sur un cluster Duckdb en lecture / √©criture.

Cependant, ce pipeline ETL permet in fine de r√©aliser des analyses simples des donn√©es des stations de v√©lo en libre service en r√©gion parisienne.

### Comment faire fonctionner ce projet?

Pour faire fonctionner ce sujet, c'est assez simple:

```bash 
git clone https://github.com/kevinl75/polytech-de-101-2025-tp-subject.git

cd polytech-de-101-2025-tp-subject

python3 -m venv .venv

source .venv/bin/activate

pip install -r requirements.txt

python src/main.py
```

## Sujet du TP

Le but de ce TP est d'enrichir ce pipeline avec les donn√©es provenant de le ville de Paris, mais aussi avec les donn√©es d'autre villes. Les sources de donn√©es disponibles sont :

- [Open data Nantes](https://data.nantesmetropole.fr/explore/dataset/244400404_stations-velos-libre-service-nantes-metropole-disponibilites/api/)

- [Open data Toulouse](https://data.toulouse-metropole.fr/explore/dataset/api-velo-toulouse-temps-reel/api/)

**L'ajout d'une seule source de donn√©es est suffisant.**

Aussi, il faut remplacer la source de donn√©es des tables `CONSOLIDATE_CITY` et `DIM_CITY` par les donn√©es provenant de l'API suivante :

- [Open data communes](https://geo.api.gouv.fr/communes)

Une fois l'acquisition de ces nouvelles donn√©es r√©alis√©e, il faut enrichir le pipeline avec les √©tapes suivantes :

- ajouter les donn√©es de la nouvelle ville dans la consolidation des tables `CONSOLIDATE_STATION` et `CONSOLIDATE_STATION_STATEMENT`

- remplacer la consolidation de `CONSOLIDATE_CITY` et l'adapter pour utiliser les donn√©es des communes r√©cup√©r√©es plus haut

- adapter si besoin les processus d'agr√©gation des tables `DIM_STATION` et `FACT_STATION_STATEMENT` et `DIM_CITY`

Au final, le pipeline ETL manager devrait ressembler √† ce qui suit :

![Process final](images/image.png)

Au final, vous devriez √™tre capable de r√©aliser les requ√™tes SQL suivantes sur votre base de donn√©es DuckDB :

```sql
-- Nb d'emplacements disponibles de v√©los dans une ville
SELECT dm.NAME, tmp.SUM_BICYCLE_DOCKS_AVAILABLE
FROM DIM_CITY dm INNER JOIN (
    SELECT CITY_ID, SUM(BICYCLE_DOCKS_AVAILABLE) AS SUM_BICYCLE_DOCKS_AVAILABLE
    FROM FACT_STATION_STATEMENT
    WHERE CREATED_DATE = (SELECT MAX(CREATED_DATE) FROM CONSOLIDATE_STATION)
    GROUP BY CITY_ID
) tmp ON dm.ID = tmp.CITY_ID
WHERE lower(dm.NAME) in ('paris', 'nantes', 'vincennes', 'toulouse');

![Pr](images/img1.png)


-- Nb de v√©los disponibles en moyenne dans chaque station
SELECT ds.name, ds.code, ds.address, tmp.avg_dock_available
FROM DIM_STATION ds JOIN (
    SELECT station_id, AVG(BICYCLE_AVAILABLE) AS avg_dock_available
    FROM FACT_STATION_STATEMENT
    GROUP BY station_id
) AS tmp ON ds.id = tmp.station_id;
```
![Pr](images/img2.png)
Vous pouvez utiliser la commande `duckdb data/duckdb/mobility_analysis.duckdb` pour ouvrir l'invite de commande DuckDB. 

Le sujet devra √™tre rendu sous la forme d'un repository GitHub avec les instructions n√©c√©ssaire pour faire fonctionner correctement le projet. Le projet peut √™tre fait seul ou en duo.

### Bar√®me utilis√© pour la notation finale :

- Les ingestions fonctionnent correctement et produisent des fichiers json localement (5 points)

- La consolidation est correctement r√©alis√© avec les nouvelles ing√©r√©es (5 points)

- L'agr√©gation des donn√©es est correctement r√©alis√©e et les requ√™tes SQL ci-dessus fonctionnent (5 points)

- Le projet int√®gre pas seulement les donn√©es de la ville de Paris mais aussi les donn√©es d'une autre ville (5 points)

- 1 points bonus pour la clart√© g√©n√©rale du code (commentaires, noms de variables, etc.)
